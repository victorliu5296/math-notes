<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.133.0"><script src="/math-notes/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=math-notes/livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Victor Liu's Math Notes</title>
<meta name=keywords content="Blog,Portfolio,PaperMod"><meta name=description content="Victor Liu's math notes"><meta name=author content="Victor Liu"><link rel=canonical href=http://localhost:1313/math-notes/><link crossorigin=anonymous href=/math-notes/assets/css/stylesheet.54405a410796490bc874ab6181fac9b675753cc2b91375d8f882566459eca428.css integrity="sha256-VEBaQQeWSQvIdKthgfrJtnV1PMK5E3XY+IJWZFnspCg=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/math-notes/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/math-notes/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/math-notes/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/math-notes/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/math-notes/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=http://localhost:1313/math-notes/index.xml><link rel=alternate hreflang=en href=http://localhost:1313/math-notes/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Victor Liu's Math Notes"><meta property="og:description" content="Victor Liu's math notes"><meta property="og:type" content="website"><meta property="og:url" content="http://localhost:1313/math-notes/"><meta property="og:site_name" content="Victor Liu's Math Notes"><meta name=twitter:card content="summary"><meta name=twitter:title content="Victor Liu's Math Notes"><meta name=twitter:description content="Victor Liu's math notes"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Victor Liu's Math Notes","url":"http://localhost:1313/math-notes/","description":"Victor Liu's math notes","thumbnailUrl":"http://localhost:1313/math-notes/favicon.ico","sameAs":["https://github.com/victorliu5296"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/math-notes/ accesskey=h title="Victor Liu's Math Notes (Alt + H)">Victor Liu's Math Notes</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/math-notes/about-me/ title="About Me"><span>About Me</span></a></li><li><a href=http://localhost:1313/math-notes/portfolio/ title=Portfolio><span>Portfolio</span></a></li></ul></nav></header><main class=main><article class="first-entry home-info"><header class=entry-header><h1>Welcome</h1></header><div class=entry-content>Hi! I&rsquo;m Victor, a high school student. I enjoy mathematics. Here, I share my notes on topics I find interesting or confusing. Check <a href=/about-me/>about me</a> for more information.</div><footer class=entry-footer><div class=social-icons><a href=https://github.com/victorliu5296 target=_blank rel="noopener noreferrer me" title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></div></footer></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>CLT and diffusion models</h2></header><div class=entry-content><p>It’s interesting to see that the two versions of the explanation are quite different, with ChatGPT’s version being more centered on intuition and Mistral Large-2’s version being more technical and mathematical.
Prompt: “Can you help me write about the connection between the Central limit theorem and the reverse process of diffusion models being able to initialize from normal distributions”
ChatGPT’s version The Central Limit Theorem and Diffusion Models Overview of the Central Limit Theorem (CLT) The Central Limit Theorem (CLT) is a fundamental principle in probability theory and statistics....</p></div><footer class=entry-footer><span title='2024-08-13 00:00:00 +0000 UTC'>August 13, 2024</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;1055 words&nbsp;·&nbsp;Victor Liu</footer><a class=entry-link aria-label="post link to CLT and diffusion models" href=http://localhost:1313/math-notes/posts/clt-and-diffusion-models/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Encoder-decoder vs decoder only transformer models</h2></header><div class=entry-content><p>Encoder-decoder vs decoder only transformer models Modern state-of-the-art Transformer models are based on the decoder-only architecture. However, I was initially confused by the name “encoder”, which to me alludes to encoding the words into numbers. This is actually the role of the embedding layer.
So, what are the differences between an encoder-deconder and decoder-only transformer model?
High-level comparison Aspect Encoder-Decoder Transformers Decoder-Only Models Architecture Separate encoder and decoder Single stack of decoder layers Components Encoder, Decoder Decoder only Input Processing Encoder processes entire input at once Sequential, one token at a time Typical Use Cases Translation, Summarization, Q&amp;A Text generation, Autocomplete, Conversational AI Attention Mechanism Self-attention + Cross-attention Self-attention with causal masking Parallelization Encoder: ParallelDecoder: Sequential Inherently sequential Directionality Encoder: BidirectionalDecoder: Unidirectional Unidirectional Training Objective Sequence-to-sequence tasks Next-token prediction Model Size Generally larger Often more compact Flexibility Better for distinct input/output sequences Simpler for open-ended generation Examples BERT+GPT hybrid, T5, BART GPT series, BLOOM, LLaMA The main difference is that encoder-decoder models distinguish the input processed by the encoder and the tokens generated by the decoder, whereas in decoder-only models, there is no distinction between input and output tokens; they are treated as the same type of tokens to predict the next token....</p></div><footer class=entry-footer><span title='2024-08-13 00:00:00 +0000 UTC'>August 13, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;439 words&nbsp;·&nbsp;Victor Liu</footer><a class=entry-link aria-label="post link to Encoder-decoder vs decoder only transformer models" href=http://localhost:1313/math-notes/posts/encoder-decoder-vs-decoder-only/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Overview of transformers</h2></header><div class=entry-content><p>Can you explain transformers, how they work and the history leading up to them
(Answer by Claude 3.5 Sonnet)
Certainly! I’d be happy to explain transformers, their functionality, and the historical context that led to their development. Let’s break this down into several sections for clarity.
What are Transformers? Transformers are a type of deep learning model architecture that has revolutionized natural language processing (NLP) and various other AI tasks. They were introduced in 2017 by Vaswani et al....</p></div><footer class=entry-footer><span title='2024-08-13 00:00:00 +0000 UTC'>August 13, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;478 words&nbsp;·&nbsp;Victor Liu</footer><a class=entry-link aria-label="post link to Overview of transformers" href=http://localhost:1313/math-notes/posts/overview-of-transformers/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Separating positional encoding and semantic information</h2></header><div class=entry-content><p>I’m curious as to the positional encoding part. I’ve seen the mathematics part, we add a positional encoding vector to a semantic encoding vector. My question is, how does the model manage to separate the 2 parts? Aren’t we losing information this way? How does it recover it meaningfully?
(Answer by Claude 3.5 Sonnet) This is an excellent and insightful question! You’re touching on a crucial aspect of transformer architectures. Let’s dive deeper into positional encoding and address your concerns....</p></div><footer class=entry-footer><span title='2024-08-13 00:00:00 +0000 UTC'>August 13, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;611 words&nbsp;·&nbsp;Victor Liu</footer><a class=entry-link aria-label="post link to Separating positional encoding and semantic information" href=http://localhost:1313/math-notes/posts/separating-positional-encoding-and-semantic-information/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>The Lottery Ticket Hypothesis</h2></header><div class=entry-content><p>The Lottery Ticket Hypothesis This chart shows a simplified 3D landscape illustrating the loss function of the neural network.
Left: The original “full” network, which is overparameterized. Right: The pruned network, which has less bumps and is cheaper to optimize. However, in some points, it can be less “certain” than the original network, which explains the higher training loss despite the higher accuracy on the test set. You can see it interpolating approximately rather than partially overfitting....</p></div><footer class=entry-footer><span title='2024-08-13 00:00:00 +0000 UTC'>August 13, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;551 words&nbsp;·&nbsp;Victor Liu</footer><a class=entry-link aria-label="post link to The Lottery Ticket Hypothesis" href=http://localhost:1313/math-notes/posts/lottery-ticket-hypothesis/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Unbiased sample variance</h2></header><div class=entry-content><p>Dividing sample variance by one less than the sample size So I was looking at the statistics course on Khan Academy, and he said that the sample variance is “biased” if we directly divide by the sample size, in the sense that it underestimates the true population variance. To compensate, we divide by one less than the sample size. I asked ChatGPT for more detailed explanations, and here is an algebraic derivation of the biased sample variance estimator using expected value....</p></div><footer class=entry-footer><span title='2024-08-13 00:00:00 +0000 UTC'>August 13, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;595 words&nbsp;·&nbsp;Victor Liu</footer><a class=entry-link aria-label="post link to Unbiased sample variance" href=http://localhost:1313/math-notes/posts/unbiased-sample-variance/></a></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/math-notes/>Victor Liu's Math Notes</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>