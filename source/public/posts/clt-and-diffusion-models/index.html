<!doctype html><html lang=en dir=auto><head><script src="/math-notes/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=math-notes/livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>CLT and diffusion models | Victor Liu's Math Notes</title>
<meta name=keywords content><meta name=description content="Connections between the Central Limit Theorem (CLT) and the reverse process of diffusion models being able to initialize from normal (Gaussian) distributions."><meta name=author content="Victor Liu"><link rel=canonical href=http://localhost:1313/math-notes/posts/clt-and-diffusion-models/><link crossorigin=anonymous href=/math-notes/assets/css/stylesheet.54405a410796490bc874ab6181fac9b675753cc2b91375d8f882566459eca428.css integrity="sha256-VEBaQQeWSQvIdKthgfrJtnV1PMK5E3XY+IJWZFnspCg=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/math-notes/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/math-notes/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/math-notes/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/math-notes/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/math-notes/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/math-notes/posts/clt-and-diffusion-models/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="CLT and diffusion models"><meta property="og:description" content="Connections between the Central Limit Theorem (CLT) and the reverse process of diffusion models being able to initialize from normal (Gaussian) distributions."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/math-notes/posts/clt-and-diffusion-models/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-13T00:00:00+00:00"><meta property="article:modified_time" content="2024-08-13T00:00:00+00:00"><meta property="og:site_name" content="Victor Liu's Math Notes"><meta name=twitter:card content="summary"><meta name=twitter:title content="CLT and diffusion models"><meta name=twitter:description content="Connections between the Central Limit Theorem (CLT) and the reverse process of diffusion models being able to initialize from normal (Gaussian) distributions."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/math-notes/posts/"},{"@type":"ListItem","position":2,"name":"CLT and diffusion models","item":"http://localhost:1313/math-notes/posts/clt-and-diffusion-models/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"CLT and diffusion models","name":"CLT and diffusion models","description":"Connections between the Central Limit Theorem (CLT) and the reverse process of diffusion models being able to initialize from normal (Gaussian) distributions.","keywords":[],"articleBody":"It’s interesting to see that the two versions of the explanation are quite different, with ChatGPT’s version being more centered on intuition and Mistral Large-2’s version being more technical and mathematical.\nPrompt: “Can you help me write about the connection between the Central limit theorem and the reverse process of diffusion models being able to initialize from normal distributions”\nChatGPT’s version The Central Limit Theorem and Diffusion Models Overview of the Central Limit Theorem (CLT) The Central Limit Theorem (CLT) is a fundamental principle in probability theory and statistics. It states that the sum (or average) of a large number of independent, identically distributed (i.i.d.) random variables, regardless of their original distribution, will tend to follow a normal (Gaussian) distribution, provided that certain conditions are met. This convergence to a normal distribution occurs as the number of variables increases.\nMathematically, if $X_1, X_2, \\dots, X_n$ are i.i.d. random variables with mean $\\mu$ and variance $\\sigma^2$, the normalized sum:\n$$ Z_n = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\left( \\frac{X_i - \\mu}{\\sigma} \\right) $$\napproaches a standard normal distribution $N(0,1)$ as $n$ becomes large.\nDiffusion Models and Their Reverse Process Diffusion models are a type of generative model used in machine learning and statistics. These models simulate a diffusion process where data points are progressively noised until they resemble a simple distribution, typically a normal distribution. The reverse process of this diffusion aims to recover the original data by starting from a sample in this simple distribution and reversing the noise.\nIn the forward process, a data point $x_0$ is incrementally noised through a series of steps:\n$$ x_t = x_{t-1} + \\text{noise} $$\nwhere $t$ indicates the time step, and the noise is often Gaussian. After many such steps, the distribution of the data converges to a normal distribution due to the CLT, because the accumulated effect of adding small, independent noise at each step approximates a normal distribution.\nIn the reverse process, the model aims to reverse this diffusion to generate data samples by denoising a sample from the normal distribution back to the original data distribution. This reverse process is crucially dependent on the fact that the forward process converges to a normal distribution.\nConnection Between CLT and the Reverse Process The connection between the CLT and the reverse process in diffusion models lies in the justification for initializing the reverse process from a normal distribution. The CLT explains why the forward process in a diffusion model, after many iterations, leads to a normal distribution, regardless of the initial data distribution. This normal distribution, reached through the forward diffusion, serves as the starting point for the reverse process.\nWhen we initialize the reverse process from a normal distribution, we’re leveraging the fact that the forward process transforms any complex data distribution into this normal form. Therefore, by starting the reverse process from a normal distribution, we can effectively “trace back” through the noise to recover a sample from the original complex distribution.\nIn summary, the CLT provides the theoretical underpinning for why the diffusion process leads to a normal distribution in the first place, and this understanding allows us to confidently start the reverse process of diffusion models from a normal distribution to generate realistic data samples.\nMistral Large-2’s version Title: The Central Limit Theorem and the Reverse Process of Diffusion Models\nIntroduction\nThe Central Limit Theorem (CLT) is a fundamental concept in probability theory and statistics, asserting that the distribution of the sum (or average) of a large number of independent, identically distributed (i.i.d.) random variables approaches a normal distribution, regardless of the original distribution. Diffusion models, on the other hand, are a class of generative models that have shown promise in various applications, including image and audio synthesis. Interestingly, the reverse process of diffusion models is intrinsically connected to the CLT, particularly when initialized from normal distributions.\nThe Central Limit Theorem\nThe CLT states that for a sequence of i.i.d. random variables $X_1, X_2, \\ldots, X_n$ with finite mean $\\mu$ and variance $\\sigma^2$, the distribution of the standardized sum\n$$ Z_n = \\frac{\\sum_{i=1}^{n} X_i - n\\mu}{\\sigma \\sqrt{n}} $$\nconverges to a standard normal distribution $N(0, 1)$ as $n \\to \\infty$. This theorem explains why the normal distribution appears so frequently in nature and forms the basis for many statistical tests and confidence intervals.\nDiffusion Models and the Reverse Process\nDiffusion models define a Markov chain that gradually adds Gaussian noise to the data, transforming it into a simple, tractable distribution, such as a standard normal distribution. The reverse process then learns to denoise the data, generating new samples from the original data distribution.\nGiven a data distribution $q(x_0)$, the forward diffusion process is defined as:\n$$ q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I) $$\nwhere $\\beta_t$ is a variance schedule. After a large number of time steps $T$, the data distribution $q(x_T)$ approaches a standard normal distribution due to the cumulative effect of the added Gaussian noise.\nThe reverse process is also a Markov chain with Gaussian transitions:\n$$ p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)) $$\nwhere $\\mu_\\theta$ and $\\Sigma_\\theta$ are parameterized neural networks that predict the mean and covariance of the Gaussian distribution, respectively.\nConnection to the Central Limit Theorem\nThe connection between the CLT and the reverse process of diffusion models lies in the initialization from a normal distribution. As the forward diffusion process involves summing independent Gaussian noise increments, the CLT guarantees that the resulting distribution after a large number of time steps will be approximately normal.\nMoreover, the reverse process can be viewed as a series of denoising steps, where each step refines the current estimate by removing some of the added noise. Starting from a normal distribution, the reverse process gradually transforms the data back to the original distribution, essentially inverting the summation of noise increments described by the CLT.\nConclusion\nThe Central Limit Theorem provides a theoretical foundation for the forward diffusion process, explaining why the data distribution approaches a normal distribution after a large number of noise additions. The reverse process of diffusion models leverages this property, initializing from a normal distribution and learning to denoise the data, ultimately generating new samples from the original data distribution. This intrinsic connection highlights the importance of the CLT in the context of diffusion models and contributes to our understanding of their underlying mechanisms.\n","wordCount":"1055","inLanguage":"en","datePublished":"2024-08-13T00:00:00Z","dateModified":"2024-08-13T00:00:00Z","author":{"@type":"Person","name":"Victor Liu"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/math-notes/posts/clt-and-diffusion-models/"},"publisher":{"@type":"Organization","name":"Victor Liu's Math Notes","logo":{"@type":"ImageObject","url":"http://localhost:1313/math-notes/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/math-notes/ accesskey=h title="Victor Liu's Math Notes (Alt + H)">Victor Liu's Math Notes</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/math-notes/about-me/ title="About Me"><span>About Me</span></a></li><li><a href=http://localhost:1313/math-notes/portfolio/ title=Portfolio><span>Portfolio</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/math-notes/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/math-notes/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">CLT and diffusion models</h1><div class=post-description>Connections between the Central Limit Theorem (CLT) and the reverse process of diffusion models being able to initialize from normal (Gaussian) distributions.</div><div class=post-meta><span title='2024-08-13 00:00:00 +0000 UTC'>August 13, 2024</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;1055 words&nbsp;·&nbsp;Victor Liu&nbsp;|&nbsp;<a href=https://github.com/victorliu5296/math-notes/source/content/posts/posts/CLT%20and%20diffusion%20models.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>It&rsquo;s interesting to see that the two versions of the explanation are quite different, with ChatGPT&rsquo;s version being more centered on intuition and Mistral Large-2&rsquo;s version being more technical and mathematical.</p><p>Prompt: &ldquo;Can you help me write about the connection between the Central limit theorem and the reverse process of diffusion models being able to initialize from normal distributions&rdquo;</p><h2 id=chatgpts-version>ChatGPT&rsquo;s version<a hidden class=anchor aria-hidden=true href=#chatgpts-version>#</a></h2><h3 id=the-central-limit-theorem-and-diffusion-models>The Central Limit Theorem and Diffusion Models<a hidden class=anchor aria-hidden=true href=#the-central-limit-theorem-and-diffusion-models>#</a></h3><h4 id=overview-of-the-central-limit-theorem-clt>Overview of the Central Limit Theorem (CLT)<a hidden class=anchor aria-hidden=true href=#overview-of-the-central-limit-theorem-clt>#</a></h4><p>The Central Limit Theorem (CLT) is a fundamental principle in probability theory and statistics. It states that the sum (or average) of a large number of independent, identically distributed (i.i.d.) random variables, regardless of their original distribution, will tend to follow a normal (Gaussian) distribution, provided that certain conditions are met. This convergence to a normal distribution occurs as the number of variables increases.</p><p>Mathematically, if $X_1, X_2, \dots, X_n$ are i.i.d. random variables with mean $\mu$ and variance $\sigma^2$, the normalized sum:</p><p>$$
Z_n = \frac{1}{\sqrt{n}} \sum_{i=1}^{n} \left( \frac{X_i - \mu}{\sigma} \right)
$$</p><p>approaches a standard normal distribution $N(0,1)$ as $n$ becomes large.</p><h4 id=diffusion-models-and-their-reverse-process>Diffusion Models and Their Reverse Process<a hidden class=anchor aria-hidden=true href=#diffusion-models-and-their-reverse-process>#</a></h4><p>Diffusion models are a type of generative model used in machine learning and statistics. These models simulate a diffusion process where data points are progressively noised until they resemble a simple distribution, typically a normal distribution. The reverse process of this diffusion aims to recover the original data by starting from a sample in this simple distribution and reversing the noise.</p><p>In the forward process, a data point $x_0$ is incrementally noised through a series of steps:</p><p>$$
x_t = x_{t-1} + \text{noise}
$$</p><p>where $t$ indicates the time step, and the noise is often Gaussian. After many such steps, the distribution of the data converges to a normal distribution due to the CLT, because the accumulated effect of adding small, independent noise at each step approximates a normal distribution.</p><p>In the reverse process, the model aims to reverse this diffusion to generate data samples by denoising a sample from the normal distribution back to the original data distribution. This reverse process is crucially dependent on the fact that the forward process converges to a normal distribution.</p><h4 id=connection-between-clt-and-the-reverse-process>Connection Between CLT and the Reverse Process<a hidden class=anchor aria-hidden=true href=#connection-between-clt-and-the-reverse-process>#</a></h4><p>The connection between the CLT and the reverse process in diffusion models lies in the justification for initializing the reverse process from a normal distribution. The CLT explains why the forward process in a diffusion model, after many iterations, leads to a normal distribution, regardless of the initial data distribution. This normal distribution, reached through the forward diffusion, serves as the starting point for the reverse process.</p><p>When we initialize the reverse process from a normal distribution, we&rsquo;re leveraging the fact that the forward process transforms any complex data distribution into this normal form. Therefore, by starting the reverse process from a normal distribution, we can effectively &ldquo;trace back&rdquo; through the noise to recover a sample from the original complex distribution.</p><p>In summary, the CLT provides the theoretical underpinning for why the diffusion process leads to a normal distribution in the first place, and this understanding allows us to confidently start the reverse process of diffusion models from a normal distribution to generate realistic data samples.</p><hr><h2 id=mistral-large-2s-version>Mistral Large-2&rsquo;s version<a hidden class=anchor aria-hidden=true href=#mistral-large-2s-version>#</a></h2><p><strong>Title: The Central Limit Theorem and the Reverse Process of Diffusion Models</strong></p><p><strong>Introduction</strong></p><p>The Central Limit Theorem (CLT) is a fundamental concept in probability theory and statistics, asserting that the distribution of the sum (or average) of a large number of independent, identically distributed (i.i.d.) random variables approaches a normal distribution, regardless of the original distribution. Diffusion models, on the other hand, are a class of generative models that have shown promise in various applications, including image and audio synthesis. Interestingly, the reverse process of diffusion models is intrinsically connected to the CLT, particularly when initialized from normal distributions.</p><p><strong>The Central Limit Theorem</strong></p><p>The CLT states that for a sequence of i.i.d. random variables $X_1, X_2, \ldots, X_n$ with finite mean $\mu$ and variance $\sigma^2$, the distribution of the standardized sum</p><p>$$
Z_n = \frac{\sum_{i=1}^{n} X_i - n\mu}{\sigma \sqrt{n}}
$$</p><p>converges to a standard normal distribution $N(0, 1)$ as $n \to \infty$. This theorem explains why the normal distribution appears so frequently in nature and forms the basis for many statistical tests and confidence intervals.</p><p><strong>Diffusion Models and the Reverse Process</strong></p><p>Diffusion models define a Markov chain that gradually adds Gaussian noise to the data, transforming it into a simple, tractable distribution, such as a standard normal distribution. The reverse process then learns to denoise the data, generating new samples from the original data distribution.</p><p>Given a data distribution $q(x_0)$, the forward diffusion process is defined as:</p><p>$$
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)
$$</p><p>where $\beta_t$ is a variance schedule. After a large number of time steps $T$, the data distribution $q(x_T)$ approaches a standard normal distribution due to the cumulative effect of the added Gaussian noise.</p><p>The reverse process is also a Markov chain with Gaussian transitions:</p><p>$$
p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
$$</p><p>where $\mu_\theta$ and $\Sigma_\theta$ are parameterized neural networks that predict the mean and covariance of the Gaussian distribution, respectively.</p><p><strong>Connection to the Central Limit Theorem</strong></p><p>The connection between the CLT and the reverse process of diffusion models lies in the initialization from a normal distribution. As the forward diffusion process involves summing independent Gaussian noise increments, the CLT guarantees that the resulting distribution after a large number of time steps will be approximately normal.</p><p>Moreover, the reverse process can be viewed as a series of denoising steps, where each step refines the current estimate by removing some of the added noise. Starting from a normal distribution, the reverse process gradually transforms the data back to the original distribution, essentially inverting the summation of noise increments described by the CLT.</p><p><strong>Conclusion</strong></p><p>The Central Limit Theorem provides a theoretical foundation for the forward diffusion process, explaining why the data distribution approaches a normal distribution after a large number of noise additions. The reverse process of diffusion models leverages this property, initializing from a normal distribution and learning to denoise the data, ultimately generating new samples from the original data distribution. This intrinsic connection highlights the importance of the CLT in the context of diffusion models and contributes to our understanding of their underlying mechanisms.</p><hr></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=http://localhost:1313/math-notes/posts/encoder-decoder-vs-decoder-only/><span class=title>Next »</span><br><span>Encoder-decoder vs decoder only transformer models</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share CLT and diffusion models on x" href="https://x.com/intent/tweet/?text=CLT%20and%20diffusion%20models&amp;url=http%3a%2f%2flocalhost%3a1313%2fmath-notes%2fposts%2fclt-and-diffusion-models%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share CLT and diffusion models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fmath-notes%2fposts%2fclt-and-diffusion-models%2f&amp;title=CLT%20and%20diffusion%20models&amp;summary=CLT%20and%20diffusion%20models&amp;source=http%3a%2f%2flocalhost%3a1313%2fmath-notes%2fposts%2fclt-and-diffusion-models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share CLT and diffusion models on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fmath-notes%2fposts%2fclt-and-diffusion-models%2f&title=CLT%20and%20diffusion%20models"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share CLT and diffusion models on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fmath-notes%2fposts%2fclt-and-diffusion-models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share CLT and diffusion models on whatsapp" href="https://api.whatsapp.com/send?text=CLT%20and%20diffusion%20models%20-%20http%3a%2f%2flocalhost%3a1313%2fmath-notes%2fposts%2fclt-and-diffusion-models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share CLT and diffusion models on telegram" href="https://telegram.me/share/url?text=CLT%20and%20diffusion%20models&amp;url=http%3a%2f%2flocalhost%3a1313%2fmath-notes%2fposts%2fclt-and-diffusion-models%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share CLT and diffusion models on ycombinator" href="https://news.ycombinator.com/submitlink?t=CLT%20and%20diffusion%20models&u=http%3a%2f%2flocalhost%3a1313%2fmath-notes%2fposts%2fclt-and-diffusion-models%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/math-notes/>Victor Liu's Math Notes</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>